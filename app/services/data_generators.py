"""
Data generators service - Handles generation of sample and real data
"""

import pandas as pd
import numpy as np
import random
import math
import json
import os
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional

# ì „ì—­ ë³€ìˆ˜ë¡œ ë§ˆìŠ¤í‚¹ëœ ë°ì´í„°í”„ë ˆì„ ì €ì¥
masking_df = None


class DataGenerators:
    """ë°ì´í„° ìƒì„± ì„œë¹„ìŠ¤"""
    
    @staticmethod
    def load_masking_data(excel_name: str = 'masking_df.xlsx') -> bool:
        """ë§ˆìŠ¤í‚¹ëœ ì—‘ì…€ ë°ì´í„° ë¡œë“œ"""
        global masking_df
        try:
            masking_df = pd.read_excel(excel_name)
            print(f"ğŸ“Š ë§ˆìŠ¤í‚¹ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {masking_df.shape[0]}í–‰ {masking_df.shape[1]}ì—´")
            print(f"ğŸ“Š ì»¬ëŸ¼ ëª©ë¡: {list(masking_df.columns)}")
            return True
        except FileNotFoundError:
            print("âš ï¸ masking_df.xlsx íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return False
        except Exception as e:
            print(f"âŒ ë§ˆìŠ¤í‚¹ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
            return False

    @staticmethod
    def generate_pcm_trend_data() -> dict:
        """PCM íŠ¸ë Œë“œ ë°ì´í„° ìƒì„±"""
        data = {}
        para_list = ["PARA1", "PARA2"]
        for para in para_list:
            single = []
            for i in range(1, 1000):
                single.append({
                    'DATE_WAFER_ID': f'2025-06-{i}:36:57:54_A12345678998999',
                    'MIN': round(random.uniform(8, 12), 2),
                    'MAX': round(random.uniform(18, 22), 2),
                    'Q1': round(random.uniform(14, 16), 2),
                    'Q2': round(random.uniform(15, 17), 2),
                    'Q3': round(random.uniform(16, 18), 2),
                    'DEVICE': random.choice(['A', 'B', 'C']),
                    'USL': 30,
                    'TGT': 15,
                    'LSL': 1,
                    'UCL': 25,
                    'LCL': 6
                })
            data[para] = single
        return data

    @staticmethod
    def generate_commonality_data() -> Tuple[List, Dict]:
        """Commonality ë°ì´í„° ìƒì„±"""
        # í…Œì´ë¸”ìš© ë°°ì—´ ë°ì´í„° ìƒì„± (PCM íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ë°°ì—´ë¡œ ë³€í™˜)
        pcm_data = DataGenerators.generate_pcm_trend_data()
        
        print(f"ğŸ” generate_commonality_data: pcm_data type = {type(pcm_data)}")
        print(f"ğŸ” generate_commonality_data: pcm_data keys = {list(pcm_data.keys()) if isinstance(pcm_data, dict) else 'not dict'}")
        
        # PARAë³„ ê°ì²´ë¥¼ ë°°ì—´ë¡œ ë³€í™˜
        table_data = []
        for para_name, para_data in pcm_data.items():
            for row in para_data:
                table_data.append({
                    **row,
                    'PARA': para_name
                })
        
        print(f"ğŸ” generate_commonality_data: table_data type = {type(table_data)}")
        print(f"ğŸ” generate_commonality_data: table_data length = {len(table_data)}")
        print(f"ğŸ” generate_commonality_data: table_data sample = {table_data[:2] if table_data else 'empty'}")
        
        # Commonality ì •ë³´
        commonality = {
            'good_lots': ['LOT001', 'LOT002', 'LOT003'],
            'bad_lots': ['LOT004', 'LOT005'],
            'good_wafers': ['WAFER001', 'WAFER002', 'WAFER003'],
            'bad_wafers': ['WAFER004', 'WAFER005']
        }
        
        return table_data, commonality

    @staticmethod
    def generate_pcm_point_data() -> List:
        """PCM íŠ¸ë Œë“œ í¬ì¸íŠ¸(ë¼ì¸+ë§ˆì»¤)ìš© ì˜ˆì‹œ ë°ì´í„° (ê³ ì •ê°’)"""
        return [
            {'DATE_WAFER_ID': '2025-06-1:36:57:54_A12345678998999', 'PCM_SITE': '1', 'VALUE': 10},
            {'DATE_WAFER_ID': '2025-06-2:36:57:54_A12345678998999', 'PCM_SITE': '2', 'VALUE': 11},
            {'DATE_WAFER_ID': '2025-06-3:36:57:54_A12345678998999', 'PCM_SITE': '3', 'VALUE': 12},
            {'DATE_WAFER_ID': '2025-06-4:36:57:54_A12345678998999', 'PCM_SITE': '4', 'VALUE': 13},
            {'DATE_WAFER_ID': '2025-06-5:36:57:54_A12345678998999', 'PCM_SITE': '5', 'VALUE': 14},
            {'DATE_WAFER_ID': '2025-06-6:36:57:54_A12345678998999', 'PCM_SITE': '1', 'VALUE': 11},
            {'DATE_WAFER_ID': '2025-06-7:36:57:54_A12345678998999', 'PCM_SITE': '2', 'VALUE': 12},
            {'DATE_WAFER_ID': '2025-06-8:36:57:54_A12345678998999', 'PCM_SITE': '3', 'VALUE': 13},
            {'DATE_WAFER_ID': '2025-06-9:36:57:54_A12345678998999', 'PCM_SITE': '4', 'VALUE': 14},
            {'DATE_WAFER_ID': '2025-06-10:36:57:54_A12345678998999', 'PCM_SITE': '5', 'VALUE': 15},
            {'DATE_WAFER_ID': '2025-06-11:36:57:54_A12345678998999', 'PCM_SITE': '1', 'VALUE': 10},
            {'DATE_WAFER_ID': '2025-06-12:36:57:54_A12345678998999', 'PCM_SITE': '2', 'VALUE': 11},
            {'DATE_WAFER_ID': '2025-06-13:36:57:54_A12345678998999', 'PCM_SITE': '3', 'VALUE': 12},
            {'DATE_WAFER_ID': '2025-06-14:36:57:54_A12345678998999', 'PCM_SITE': '4', 'VALUE': 13},
            {'DATE_WAFER_ID': '2025-06-15:36:57:54_A12345678998999', 'PCM_SITE': '5', 'VALUE': 14},
            {'DATE_WAFER_ID': '2025-06-16:36:57:54_A12345678998999', 'PCM_SITE': '1', 'VALUE': 12},
            {'DATE_WAFER_ID': '2025-06-17:36:57:54_A12345678998999', 'PCM_SITE': '2', 'VALUE': 13},
            {'DATE_WAFER_ID': '2025-06-18:36:57:54_A12345678998999', 'PCM_SITE': '3', 'VALUE': 14},
            {'DATE_WAFER_ID': '2025-06-19:36:57:54_A12345678998999', 'PCM_SITE': '4', 'VALUE': 15},
            {'DATE_WAFER_ID': '2025-06-20:36:57:54_A12345678998999', 'PCM_SITE': '5', 'VALUE': 16},
            {'DATE_WAFER_ID': '2025-06-21:36:57:54_A12345678998999', 'PCM_SITE': '1', 'VALUE': 14},
            {'DATE_WAFER_ID': '2025-06-22:36:57:54_A12345678998999', 'PCM_SITE': '2', 'VALUE': 13},
            {'DATE_WAFER_ID': '2025-06-23:36:57:54_A12345678998999', 'PCM_SITE': '3', 'VALUE': 13},
            {'DATE_WAFER_ID': '2025-06-24:36:57:54_A12345678998999', 'PCM_SITE': '4', 'VALUE': 12},
            {'DATE_WAFER_ID': '2025-06-25:36:57:54_A12345678998999', 'PCM_SITE': '5', 'VALUE': 11},
        ]

    @staticmethod
    def generate_inline_analysis_data() -> List:
        """INLINE ë¶„ì„ ë°ì´í„° ìƒì„±"""
        # ë§ˆìŠ¤í‚¹ëœ ì—‘ì…€ ë°ì´í„° ë¡œë“œ ì‹œë„
        DataGenerators.load_masking_data(excel_name='iqc_data.xlsx')
        global masking_df

        # ì‹¤ì œ ì—‘ì…€ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
        if masking_df is not None and not masking_df.empty:
            try:
                print("ğŸ“Š ì‹¤ì œ ë§ˆìŠ¤í‚¹ ë°ì´í„° ì‚¬ìš©")
                
                # ë°ì´í„°í”„ë ˆì„ ë³µì‚¬ í›„ ì •ë¦¬
                df_clean = masking_df.copy()
                
                # 1. datetime/timestamp ì»¬ëŸ¼ë“¤ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
                for col in df_clean.columns:
                    if df_clean[col].dtype == 'datetime64[ns]' or pd.api.types.is_datetime64_any_dtype(df_clean[col]):
                        df_clean[col] = df_clean[col].dt.strftime('%Y-%m-%d %H:%M:%S')
                        print(f"ğŸ“… ë‚ ì§œ ì»¬ëŸ¼ ë³€í™˜: {col}")
                    elif df_clean[col].dtype == 'object':
                        # object íƒ€ì… ì»¬ëŸ¼ì—ì„œ ìˆ¨ê²¨ì§„ Timestamp ì°¾ê¸°
                        sample_val = df_clean[col].dropna().iloc[0] if len(df_clean[col].dropna()) > 0 else None
                        if sample_val is not None and isinstance(sample_val, (pd.Timestamp, datetime)):
                            df_clean[col] = df_clean[col].apply(
                                lambda x: x.strftime('%Y-%m-%d %H:%M:%S') if pd.notna(x) and isinstance(x, (pd.Timestamp, datetime)) else x
                            )
                            print(f"ğŸ“… ìˆ¨ê²¨ì§„ ë‚ ì§œ ì»¬ëŸ¼ ë³€í™˜: {col}")
                
                # 2. NaN, inf, -inf ê°’ë“¤ì„ Noneìœ¼ë¡œ ë³€í™˜ (ì¤‘ìš”!)
                df_clean = df_clean.replace([np.nan, np.inf, -np.inf], None)
                
                # 3. numpy íƒ€ì…ë“¤ì„ Python ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
                for col in df_clean.columns:
                    if df_clean[col].dtype == 'int64':
                        df_clean[col] = df_clean[col].astype('Int64')  # nullable integer
                    elif df_clean[col].dtype == 'float64':
                        pass  # floatì€ ê·¸ëŒ€ë¡œ

                # 4. ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                data = df_clean.to_dict(orient='records')
                
                # 5. ê° ë ˆì½”ë“œì—ì„œ ëª¨ë“  Timestamp ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
                for record in data:
                    for key, value in list(record.items()):
                        if isinstance(value, float) and (math.isnan(value) or math.isinf(value)):
                            record[key] = None
                        elif isinstance(value, (pd.Timestamp, datetime)):
                            record[key] = value.strftime('%Y-%m-%d %H:%M:%S')
                            print(f"ğŸš¨ ë ˆì½”ë“œ ë‚´ Timestamp ë³€í™˜: {key}")
                        elif key.startswith('NO_VAL') and value == 9:
                            record[key] = None
                        elif hasattr(value, 'item'):  # numpy scalar types
                            record[key] = value.item()
                
                # 6. JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸
                try:
                    json.dumps(data[0] if data else {}, default=str)
                    print("âœ… JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸ í†µê³¼")
                except Exception as json_error:
                    print(f"ğŸš¨ JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {json_error}")
                    # ë¬¸ì œê°€ ìˆëŠ” ê°’ë“¤ì„ ëª¨ë‘ ë¬¸ìì—´ë¡œ ë³€í™˜
                    for record in data:
                        for key, value in list(record.items()):
                            try:
                                json.dumps(value)
                            except:
                                print(f"ğŸ”§ ë¬¸ì œ ê°’ ìˆ˜ì •: {key} = {type(value)} -> str")
                                record[key] = str(value) if value is not None else None
                
                print(f"âœ… ë°ì´í„° ë³€í™˜ ì™„ë£Œ: {len(data)}ê°œ ë ˆì½”ë“œ")
                print(f"ğŸ“Š ì‹¤ì œ ë°ì´í„° ì»¬ëŸ¼: {list(df_clean.columns)}")
                if len(data) > 0:
                    print(f"ğŸ“Š ì²« ë²ˆì§¸ í–‰ ìƒ˜í”Œ: {data[0]}")
                
                return data
                
            except Exception as e:
                print(f"âŒ ì‹¤ì œ ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                print("ğŸ“Š ìƒ˜í”Œ ë°ì´í„°ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        
        print("ğŸ“Š ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì—‘ì…€ íŒŒì¼ ì—†ìŒ)")
        data = []
        for i in range(1, 16):
            data.append({
                'timestamp': f'2024-01-{i:02d}',
                'critical_path_length': round(random.uniform(10, 20), 2),
                'performance_score': round(random.uniform(0.7, 0.95), 3),
                'bottleneck_count': random.randint(1, 5),
                'optimization_potential': round(random.uniform(0.1, 0.3), 3)
            })
        return data

    @staticmethod
    def generate_inline_trend_initial_data() -> List:
        """INLINE Trend Initial ë°ì´í„° ìƒì„± (DEVICE ê¸°ì¤€)"""
        DataGenerators.load_masking_data(excel_name='iqc_data.xlsx')
        global masking_df

        # ì‹¤ì œ ì—‘ì…€ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
        if masking_df is not None and not masking_df.empty:
            try:
                print("ğŸ“Š ì‹¤ì œ ë§ˆìŠ¤í‚¹ ë°ì´í„° ì‚¬ìš©")
                
                # ë°ì´í„°í”„ë ˆì„ ë³µì‚¬ í›„ ì •ë¦¬
                df_clean = masking_df.copy()
                
                # 1. datetime/timestamp ì»¬ëŸ¼ë“¤ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
                for col in df_clean.columns:
                    if df_clean[col].dtype == 'datetime64[ns]' or pd.api.types.is_datetime64_any_dtype(df_clean[col]):
                        df_clean[col] = df_clean[col].dt.strftime('%Y-%m-%d %H:%M:%S')
                        print(f"ğŸ“… ë‚ ì§œ ì»¬ëŸ¼ ë³€í™˜: {col}")
                
                # 2. NaN, inf, -inf ê°’ë“¤ì„ Noneìœ¼ë¡œ ë³€í™˜ (ì¤‘ìš”!)
                df_clean = df_clean.replace([np.nan, np.inf, -np.inf], None)
                
                # 3. numpy íƒ€ì…ë“¤ì„ Python ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
                for col in df_clean.columns:
                    if df_clean[col].dtype == 'int64':
                        df_clean[col] = df_clean[col].astype('Int64')  # nullable integer
                    elif df_clean[col].dtype == 'float64':
                        pass  # floatì€ ê·¸ëŒ€ë¡œ

                # 4. (ìˆ˜ì •) FOR_KEY ë‹¨ì¼ í•„í„°ë§ ì œê±° â€” ëª¨ë“  FOR_KEY ìœ ì§€
                if 'FOR_KEY' in df_clean.columns:
                    df_clean['FOR_KEY'] = df_clean['FOR_KEY'].astype(str).str.strip()
                    uniq = df_clean['FOR_KEY'].dropna().unique().tolist()
                    print(f"ğŸ“ FOR_KEY ê³ ìœ ê°’ ìˆ˜: {len(uniq)} | ì˜ˆì‹œ: {uniq[:5]}")

                # 5. xì¶•ìš© key ì»¬ëŸ¼ ìƒì„±
                if 'TRANS_DATE' in df_clean.columns:
                    df_clean['_sort_ts'] = pd.to_datetime(df_clean['TRANS_DATE'], errors='coerce')
                    df_clean = df_clean.sort_values('_sort_ts')
                    df_clean['key'] = df_clean['_sort_ts'].dt.strftime('%Y-%m-%d %H:%M:%S').fillna(df_clean['TRANS_DATE'].astype(str))
                    df_clean = df_clean.drop(columns=['_sort_ts'])
                    print("ğŸ“ TRANS_DATEë¥¼ keyë¡œ ì‚¬ìš©")
                
                # 6. ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ í›„ ë‹¤ì‹œ í•œë²ˆ NaN ì²´í¬
                data = df_clean.to_dict(orient='records')
                
                # 7. ê° ë ˆì½”ë“œì—ì„œ NaN/inf/ê²°ì¸¡ì½”ë“œ ì²˜ë¦¬ ë° key ë¬¸ìì—´ ë³´ì¥
                for record in data:
                    for key, value in record.items():
                        if isinstance(value, float) and (math.isnan(value) or math.isinf(value)):
                            record[key] = None
                        elif key.startswith('NO_VAL') and value == 9:
                            record[key] = None
                    if 'key' in record and record['key'] is not None:
                        record['key'] = str(record['key'])
                
                print(f"âœ… ë°ì´í„° ë³€í™˜ ì™„ë£Œ: {len(data)}ê°œ ë ˆì½”ë“œ")
                print(f"ğŸ“Š ì‹¤ì œ ë°ì´í„° ì»¬ëŸ¼: {list(df_clean.columns)}")
                if len(data) > 0:
                    print(f"ğŸ“Š ì²« ë²ˆì§¸ í–‰ ìƒ˜í”Œ: {data[0]}")
                
                return data
                
            except Exception as e:
                print(f"âŒ ì‹¤ì œ ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                print("ğŸ“Š ìƒ˜í”Œ ë°ì´í„°ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        
        # (ìƒ˜í”Œ ë°ì´í„° ìƒì„±ë¶€ëŠ” ê¸°ì¡´ ê·¸ëŒ€ë¡œ ë‘ )
        print("ğŸ“Š ë°•ìŠ¤í”Œë¡¯ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±")
        data = []
        devices = ['DEVICE_A', 'DEVICE_B', 'DEVICE_C']
        for i, device in enumerate(devices):
            base_values = [380 + i*20, 420 + i*20, 460 + i*20]
            for j in range(5):
                key_idx = i * 5 + j + 1
                data.append({
                    'key': f'2024-01-{key_idx:02d} 10:00:00',
                    'FOR_KEY': 'P1931.52926.5',  # ìƒ˜í”Œì€ ê·¸ëŒ€ë¡œ
                    'DEVICE': device,
                    'NO_VAL1': round(base_values[0] + random.uniform(-30, 30), 3),
                    'NO_VAL2': round(base_values[1] + random.uniform(-30, 30), 3), 
                    'NO_VAL3': round(base_values[2] + random.uniform(-30, 30), 3),
                    'NO_VAL4': round(base_values[0] + random.uniform(-25, 25), 3),
                    'NO_VAL5': round(base_values[1] + random.uniform(-25, 25), 3),
                    'USL': 550,
                    'LSL': 300,
                    'TGT': 420,
                    'UCL': 500,
                    'LCL': 350
                })
        return data

    @staticmethod
    def generate_inline_trend_followup_data(criteria: str) -> List:
        """INLINE Trend Followup ë°ì´í„° ìƒì„± (ë‹¤ì–‘í•œ criteria ê¸°ì¤€)"""
        DataGenerators.load_masking_data(excel_name='iqc_data.xlsx')
        global masking_df

        # ì‹¤ì œ ì—‘ì…€ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
        if masking_df is not None and not masking_df.empty:
            try:
                print(f"ğŸ“Š ì‹¤ì œ ë§ˆìŠ¤í‚¹ ë°ì´í„° ì‚¬ìš© (criteria: {criteria})")
                
                df_clean = masking_df.copy()
                
                # 1. datetime/timestamp ì»¬ëŸ¼ë“¤ì„ ë¬¸ìì—´ë¡œ ë³€í™˜ (ê°•í™”)
                for col in df_clean.columns:
                    if df_clean[col].dtype == 'datetime64[ns]' or pd.api.types.is_datetime64_any_dtype(df_clean[col]):
                        df_clean[col] = df_clean[col].dt.strftime('%Y-%m-%d %H:%M:%S')
                        print(f"ğŸ“… ë‚ ì§œ ì»¬ëŸ¼ ë³€í™˜: {col}")
                    elif df_clean[col].dtype == 'object':
                        sample_val = df_clean[col].dropna().iloc[0] if len(df_clean[col].dropna()) > 0 else None
                        if sample_val is not None and isinstance(sample_val, (pd.Timestamp, datetime)):
                            df_clean[col] = df_clean[col].apply(
                                lambda x: x.strftime('%Y-%m-%d %H:%M:%S') if pd.notna(x) and isinstance(x, (pd.Timestamp, datetime)) else x
                            )
                            print(f"ğŸ“… ìˆ¨ê²¨ì§„ ë‚ ì§œ ì»¬ëŸ¼ ë³€í™˜: {col}")
                
                # 2. NaN, inf, -inf â†’ None
                df_clean = df_clean.replace([np.nan, np.inf, -np.inf], None)
                
                # 3. numpy ìŠ¤ì¹¼ë¼ íƒ€ì… ì²˜ë¦¬
                for col in df_clean.columns:
                    if df_clean[col].dtype == 'int64':
                        df_clean[col] = df_clean[col].astype('Int64')
                    elif df_clean[col].dtype == 'float64':
                        pass

                # 4. criteria ë³´ì •
                if criteria not in df_clean.columns:
                    print(f"âš ï¸ criteria '{criteria}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df_clean.columns)}")
                    available_criteria = ['MAIN_EQ', 'DEVICE', 'PARA', 'EQ_CHAM', 'LOT_ID', 'OPER', 'ROUTE']
                    for alt_criteria in available_criteria:
                        if alt_criteria in df_clean.columns:
                            print(f"ğŸ“ ëŒ€ì²´ criteria ì‚¬ìš©: {alt_criteria}")
                            criteria = alt_criteria
                            break
                    else:
                        raise ValueError("No valid criteria found")
                
                # 5. (ìˆ˜ì •) FOR_KEY ë‹¨ì¼ í•„í„°ë§ ì œê±° â€” ëª¨ë“  FOR_KEY ìœ ì§€
                if 'FOR_KEY' in df_clean.columns:
                    df_clean['FOR_KEY'] = df_clean['FOR_KEY'].astype(str).str.strip()
                    uniq = df_clean['FOR_KEY'].dropna().unique().tolist()
                    print(f"ğŸ“ FOR_KEY ê³ ìœ ê°’ ìˆ˜: {len(uniq)} | ì˜ˆì‹œ: {uniq[:5]}")

                # 6. xì¶•ìš© key ì»¬ëŸ¼ ìƒì„±
                if 'TRANS_DATE' in df_clean.columns:
                    df_clean['_sort_ts'] = pd.to_datetime(df_clean['TRANS_DATE'], errors='coerce')
                    df_clean = df_clean.sort_values('_sort_ts')
                    df_clean['key'] = df_clean['_sort_ts'].dt.strftime('%Y-%m-%d %H:%M:%S').fillna(df_clean['TRANS_DATE'].astype(str))
                    df_clean = df_clean.drop(columns=['_sort_ts'])
                    print("ğŸ“ TRANS_DATEë¥¼ keyë¡œ ì‚¬ìš©")
                elif 'LOT_NO' in df_clean.columns:
                    df_clean['key'] = df_clean['LOT_NO'].astype(str)
                    print("ğŸ“ LOT_NOë¥¼ keyë¡œ ì‚¬ìš©")
                elif 'WAFER_ID' in df_clean.columns:
                    df_clean['key'] = df_clean['WAFER_ID'].astype(str)
                    print("ğŸ“ WAFER_IDë¥¼ keyë¡œ ì‚¬ìš©")
                elif 'FOR_KEY' in df_clean.columns:
                    df_clean['key'] = df_clean['FOR_KEY'].astype(str)
                    print("ğŸ“ FOR_KEYë¥¼ keyë¡œ ì‚¬ìš©")
                else:
                    df_clean['key'] = df_clean.reset_index().index.astype(str)
                    print("ğŸ“ ì¸ë±ìŠ¤ë¥¼ keyë¡œ ì‚¬ìš©")
                
                # ğŸš¨ criteria ì»¬ëŸ¼ Timestamp íŠ¹ë³„ ì²˜ë¦¬
                if criteria in df_clean.columns:
                    print(f"ğŸ” criteria '{criteria}' ì»¬ëŸ¼ íƒ€ì…: {df_clean[criteria].dtype}")
                    sample_vals = df_clean[criteria].dropna().head(3).tolist()
                    print(f"ğŸ” criteria ìƒ˜í”Œ ê°’ë“¤: {sample_vals}")
                    for i, val in enumerate(sample_vals):
                        print(f"  {i}: {val} (type: {type(val)})")
                        if isinstance(val, (pd.Timestamp, datetime)):
                            df_clean[criteria] = df_clean[criteria].apply(
                                lambda x: x.strftime('%Y-%m-%d %H:%M:%S') if pd.notna(x) and isinstance(x, (pd.Timestamp, datetime)) else x
                            )
                            print(f"âœ… criteria '{criteria}' Timestamp â†’ ë¬¸ìì—´ ë³€í™˜ ì™„ë£Œ")
                            break
                
                # 7. ì •ë ¬
                try:
                    print(f"ğŸ“Š ì •ë ¬ ì „ criteria '{criteria}' ê³ ìœ ê°’: {df_clean[criteria].nunique()}ê°œ")
                    for idx, val in df_clean[criteria].items():
                        if isinstance(val, (pd.Timestamp, datetime)):
                            df_clean.at[idx, criteria] = val.strftime('%Y-%m-%d %H:%M:%S')
                            print(f"ğŸ”§ ì •ë ¬ ì „ ì¶”ê°€ Timestamp ìˆ˜ì •: {idx}")
                    df_clean = df_clean.sort_values([criteria, 'key'])
                    print(f"ğŸ“Š {criteria} ê¸°ì¤€ ì •ë ¬ ì™„ë£Œ")
                except Exception as e:
                    print(f"âš ï¸ ì •ë ¬ ì‹¤íŒ¨: {e}, ê¸°ë³¸ ìˆœì„œ ìœ ì§€")
                
                # 8. ë”•ì…”ë„ˆë¦¬ ë³€í™˜
                data = df_clean.to_dict(orient='records')
                
                # 9. JSON ë¹„í˜¸í™˜ ê°’ ì •ë¦¬
                for record in data:
                    for key, value in list(record.items()):
                        if isinstance(value, float) and (math.isnan(value) or math.isinf(value)):
                            record[key] = None
                        elif isinstance(value, (pd.Timestamp, datetime)):
                            record[key] = value.strftime('%Y-%m-%d %H:%M:%S')
                            print(f"ğŸš¨ ëŠ¦ê²Œ ë°œê²¬ëœ Timestamp ë³€í™˜: {key}")
                        elif key.startswith('NO_VAL') and value == 9:
                            record[key] = None
                        elif hasattr(value, 'item'):
                            record[key] = value.item()
                    if 'key' in record and record['key'] is not None:
                        record['key'] = str(record['key'])
                
                print(f"âœ… ë°ì´í„° ë³€í™˜ ì™„ë£Œ: {len(data)}ê°œ ë ˆì½”ë“œ")
                print(f"ğŸ“Š ì‚¬ìš©ëœ criteria: {criteria}")
                
                # JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸
                try:
                    json.dumps(data[0] if data else {}, default=str)
                    print("âœ… JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸ í†µê³¼")
                except Exception as json_error:
                    print(f"ğŸš¨ JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {json_error}")
                    for record in data:
                        for key, value in list(record.items()):
                            try:
                                json.dumps(value)
                            except:
                                print(f"ğŸ”§ ë¬¸ì œ ê°’ ìˆ˜ì •: {key} = {type(value)} -> str")
                                record[key] = str(value) if value is not None else None
                
                print(f"ğŸ“Š {criteria} ê³ ìœ ê°’: {sorted(list(set([r[criteria] for r in data if r.get(criteria) is not None])))}")
                return data
                
            except Exception as e:
                print(f"âŒ ì‹¤ì œ ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                print("ğŸ“Š ìƒ˜í”Œ ë°ì´í„°ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        
        # (ìƒ˜í”Œ ë°ì´í„° ìƒì„±ë¶€ëŠ” ê¸°ì¡´ ê·¸ëŒ€ë¡œ ë‘ )
        print(f"ğŸ“Š ë°•ìŠ¤í”Œë¡¯ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì¤‘ (criteria: {criteria})")
        data = []
        if criteria == "MAIN_EQ":
            criteria_values = ['EQ_001', 'EQ_002', 'EQ_003', 'EQ_004']
        elif criteria == "PARA":
            criteria_values = ['PARA_X', 'PARA_Y', 'PARA_Z']
        elif criteria == "EQ_CHAM":
            criteria_values = ['P0', 'P1', 'P2', 'P3']
        elif criteria == "OPER":
            criteria_values = ['OPER_1', 'OPER_2', 'OPER_3']
        elif criteria == "ROUTE":
            criteria_values = ['ROUTE_A', 'ROUTE_B', 'ROUTE_C']
        else:
            criteria_values = ['DEVICE_A', 'DEVICE_B', 'DEVICE_C']
        
        for i, criteria_val in enumerate(criteria_values):
            base_values = [380 + i*20, 420 + i*20, 460 + i*20]
            for j in range(5):
                key_idx = i * 5 + j + 1
                data.append({
                    'key': f'2024-01-{key_idx:02d} 10:00:00',
                    'FOR_KEY': 'P1931.52926.5',  # ìƒ˜í”Œì€ ê·¸ëŒ€ë¡œ
                    criteria: criteria_val,
                    'NO_VAL1': round(base_values[0] + random.uniform(-30, 30), 3),
                    'NO_VAL2': round(base_values[1] + random.uniform(-30, 30), 3),
                    'NO_VAL3': round(base_values[2] + random.uniform(-30, 30), 3),
                    'NO_VAL4': round(base_values[0] + random.uniform(-25, 25), 3),
                    'NO_VAL5': round(base_values[1] + random.uniform(-25, 25), 3),
                    'USL': 550,
                    'LSL': 300,
                    'TGT': 420,
                    'UCL': 500,
                    'LCL': 350
                })
        return data

    @staticmethod
    def generate_rag_search_data() -> Dict:
        """RAG ê²€ìƒ‰ ë°ì´í„° ìƒì„±"""
        return {
            'query': 'PCM ë°ì´í„° ë¶„ì„',
            'results': [
                {'title': 'PCM íŠ¸ë Œë“œ ë¶„ì„ ê°€ì´ë“œ', 'relevance': 0.95, 'content': 'PCM ë°ì´í„°ì˜ íŠ¸ë Œë“œ ë¶„ì„ ë°©ë²•...'},
                {'title': 'Commonality ë¶„ì„ ê¸°ë²•', 'relevance': 0.88, 'content': 'Commonality ë¶„ì„ì„ í†µí•œ í’ˆì§ˆ ê´€ë¦¬...'},
                {'title': 'ë°ì´í„° ì‹œê°í™” ëª¨ë²” ì‚¬ë¡€', 'relevance': 0.82, 'content': 'íš¨ê³¼ì ì¸ ë°ì´í„° ì‹œê°í™” ë°©ë²•...'}
            ],
            'total_results': 15,
            'search_time': 0.23
        }

    @staticmethod
    def generate_rag_answer_data() -> List:
        return [
            {
                "file_name": "example1.pdf",
                "file_path": "/static/docs/example1.pdf",
                "similarity": 0.98
            },
            {
                "file_name": "example2.pdf",
                "file_path": "/static/docs/example2.pdf",
                "similarity": 0.92
            },
            {
                "file_name": "example3.pdf",
                "file_path": "/static/docs/example3.pdf",
                "similarity": 0.89
            }
        ]

    @staticmethod
    def generate_pcm_to_trend_data() -> Dict:
        """PCM To Trend ë°ì´í„° ìƒì„± (ì‹¤ì œ ë§ˆìŠ¤í‚¹ëœ ì—‘ì…€ ë°ì´í„° ë˜ëŠ” ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©)"""
        # ë§ˆìŠ¤í‚¹ëœ ì—‘ì…€ ë°ì´í„° ë¡œë“œ ì‹œë„
        DataGenerators.load_masking_data(excel_name='masking_df.xlsx')
        
        global masking_df
        
        # ì‹¤ì œ ì—‘ì…€ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
        if masking_df is not None and not masking_df.empty:
            print("ğŸ“Š ì‹¤ì œ ë§ˆìŠ¤í‚¹ ë°ì´í„° ì‚¬ìš©")
            data = {}
            
            # PARA ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
            if 'PARA' in masking_df.columns:
                # PARAë³„ë¡œ ë°ì´í„° ê·¸ë£¹í™”
                para_groups = masking_df.groupby('PARA')
                for para_name, para_data in para_groups:
                    # ë°ì´í„°í”„ë ˆì„ì„ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                    data[para_name] = para_data.to_dict('records')
                    print(f"ğŸ“Š PARA {para_name}: {len(para_data)}ê°œ ë ˆì½”ë“œ")
            else:
                # PARA ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì „ì²´ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ ê·¸ë£¹ìœ¼ë¡œ ì²˜ë¦¬
                data['ALL_DATA'] = masking_df.to_dict('records')
                print(f"ğŸ“Š ì „ì²´ ë°ì´í„°: {len(masking_df)}ê°œ ë ˆì½”ë“œ")
            
            return data
        
        # ì—‘ì…€ íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒ˜í”Œ ë°ì´í„° ìƒì„±
        print("ğŸ“Š ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì—‘ì…€ íŒŒì¼ ì—†ìŒ)")
        data = {}
        para_list = ["PARA_A", "PARA_B", "PARA_C"]
        route_list = ["route1", "route2", "route3"]
        oper_list = ["oper1", "oper2", "oper3", "oper4"]
        
        for para in para_list:
            single = []
            for i in range(1, 16):  # 15ê°œ ìŠ¤í… ë°ì´í„° ìƒì„±
                # ì‹¤ì œ ë°ì´í„° êµ¬ì¡°ì™€ ë™ì¼í•œ ë²”ìœ„ë¡œ ê°’ ìƒì„±
                min_val = round(random.uniform(350, 450), 4)
                max_val = round(random.uniform(600, 700), 4)
                q1_val = round(random.uniform(min_val + 30, min_val + 80), 4)
                q2_val = round(random.uniform(q1_val + 30, q1_val + 80), 4)
                q3_val = round(random.uniform(q2_val + 30, max_val - 30), 4)
                
                single.append({
                    # ë§ˆìŠ¤í‚¹ëœ ì»¬ëŸ¼ë“¤ (ì‹¤ì œë¡œëŠ” IDë‚˜ ì¸ë±ìŠ¤ ì •ë³´)
                    'Unnamed: 0.1': i,  # ë§ˆìŠ¤í‚¹ëœ ì»¬ëŸ¼ 1
                    'Unnamed: 0': i,    # ë§ˆìŠ¤í‚¹ëœ ì»¬ëŸ¼ 2
                    
                    # ì‹¤ì œ ë°ì´í„° ì»¬ëŸ¼ë“¤
                    'key': f'{i}',  # ì‹¤ì œ ë°ì´í„°ì—ì„œëŠ” ìˆ«ì í˜•íƒœ
                    'MAIN_ROUTE_DESC': random.choice(route_list),
                    'MAIN_OPER_DESC': random.choice(oper_list),
                    'EQ_CHAM': random.choice(['P0', 'P1', 'P2']),
                    'PARA': para,
                    
                    # í†µê³„ê°’ë“¤ (ì‹¤ì œ ë°ì´í„° ë²”ìœ„ ë°˜ì˜)
                    'MIN': min_val,
                    'MAX': max_val,
                    'Q1': q1_val,
                    'Q2': q2_val,
                    'Q3': q3_val,
                    
                    # ì œì–´ì„ ë“¤ (ì‹¤ì œ ë°ì´í„° ë²”ìœ„ ë°˜ì˜)
                    'USL': 550,
                    'TGT': 420,
                    'LSL': 300,
                    'UCL': 500,
                    'LCL': 360
                })
            data[para] = single
        
        # PARAë³„ë¡œ ë¶„ë¦¬ëœ ë°ì´í„° ë°˜í™˜
        return data

    @staticmethod
    def generate_two_tables_data(test_empty_scenario: str = None) -> Dict:
        """Two Tables ë°ì´í„° ìƒì„± - ì„œë¡œ ë‹¤ë¥¸ ì»¬ëŸ¼ê³¼ ë°ì´í„°ë¥¼ ê°€ì§„ ë‘ ê°œì˜ í…Œì´ë¸”"""
        # ë§ˆìŠ¤í‚¹ëœ ì—‘ì…€ ë°ì´í„° ë¡œë“œ ì‹œë„
        DataGenerators.load_masking_data(excel_name='masking_df.xlsx')
        global masking_df
        
        # ì²« ë²ˆì§¸ í…Œì´ë¸”: Lot Hold ë°ì´í„° (ê°€ìƒì˜ lot hold ì •ë³´)
        lot_hold_data = []
        for i in range(1, 16):
            lot_hold_data.append({
                'LOT_ID': f'LOT_{i:03d}',
                'HOLD_REASON': random.choice(['QUALITY_ISSUE', 'EQUIPMENT_MAINT', 'MATERIAL_SHORTAGE', 'PROCESS_DEVIATION']),
                'HOLD_DATE': f'2024-12-{random.randint(1, 31):02d}',
                'HOLD_STATUS': random.choice(['ACTIVE', 'RELEASED', 'CANCELLED']),
                'PRIORITY': random.choice(['HIGH', 'MEDIUM', 'LOW']),
                'WAFER_COUNT': random.randint(10, 25),
                'AFFECTED_STEP': random.choice(['PHOTO', 'ETCH', 'DIFFUSION', 'METAL']),
                'OWNER': random.choice(['ENGINEER_A', 'ENGINEER_B', 'ENGINEER_C'])
            })
        
        # ë‘ ë²ˆì§¸ í…Œì´ë¸”: PE Confirm Module ë°ì´í„° (ê°€ìƒì˜ PE í™•ì¸ ëª¨ë“ˆ ì •ë³´)
        pe_confirm_data = []
        for i in range(1, 12):  # ë‹¤ë¥¸ ê°œìˆ˜ë¡œ ì„¤ì •
            pe_confirm_data.append({
                'MODULE_ID': f'PE_MOD_{i:02d}',
                'CONFIRM_STATUS': random.choice(['CONFIRMED', 'PENDING', 'REJECTED']),
                'CONFIRM_DATE': f'2024-12-{random.randint(1, 31):02d}',
                'PARAMETER_NAME': random.choice(['TEMPERATURE', 'PRESSURE', 'FLOW_RATE', 'POWER']),
                'TARGET_VALUE': round(random.uniform(100, 500), 2),
                'ACTUAL_VALUE': round(random.uniform(95, 505), 2),
                'TOLERANCE': round(random.uniform(5, 15), 1),
                'RESULT': random.choice(['PASS', 'FAIL', 'WARNING']),
                'INSPECTOR': random.choice(['INSPECTOR_X', 'INSPECTOR_Y', 'INSPECTOR_Z'])
            })
        
        # ì‹¤ì œ ì—‘ì…€ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ í…Œì´ë¸”ì— í™œìš©
        if masking_df is not None and not masking_df.empty:
            # ì‹¤ì œ ë°ì´í„°ì˜ ì¼ë¶€ë¥¼ ì²« ë²ˆì§¸ í…Œì´ë¸”ë¡œ ì‚¬ìš© (ìµœëŒ€ 10ê°œ ë ˆì½”ë“œ)
            sample_size = min(10, len(masking_df))
            lot_hold_data = masking_df.head(sample_size).to_dict('records')
            print(f"ğŸ“Š Using real data for lot_hold: {sample_size} records")
        
        # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ì²˜ë¦¬ (íŠ¹ë³„í•œ ê²½ìš°ì—ë§Œ)
        if test_empty_scenario:
            if test_empty_scenario == 'empty_lot_hold':
                lot_hold_data = []
                print("ğŸ”„ Test scenario: Empty lot_hold data")
            elif test_empty_scenario == 'empty_pe_confirm':
                pe_confirm_data = []
                print("ğŸ”„ Test scenario: Empty pe_confirm data")
            elif test_empty_scenario == 'both_empty':
                lot_hold_data = []
                pe_confirm_data = []
                print("ğŸ”„ Test scenario: Both tables empty")
        
        print(f"ğŸ“Š Generated data - Lot Hold: {len(lot_hold_data)} records, PE Confirm: {len(pe_confirm_data)} records")
        
        return {
            "result": "lot_hold_pe_confirm_module",
            "real_data": [
                {"lot_hold_module": lot_hold_data},
                {"pe_confirm_module": pe_confirm_data}
            ]
        }
